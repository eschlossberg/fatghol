
\chapter[Parallel Gaussian Elimination]
  {A novel parallel algorithm for exact Gaussian Elimination 
    of general sparse matrices}
\label{chap:rheinfall}

The algorithm presented in Chapter~\ref{chap:algorithm} reduces
computation of the Betti number of moduli spaces $\M_{g,n}$ to
reckoning the rank (over $\setQ$) of some large sparse matrix with
integer entries.  An effective method for computing this rank is given
by Gaussian Elimination.

The serial algorithm for Gaussian Elimination is well-known; it
consists of a certain number of iterations of the following two steps: a
\emph{pivoting} step followed by an \emph{elimination} step.  Starting
with the upper left entry, a non-zero element (pivot) is searched for;
once a pivot has been found, a permutation is applied so that the
pivot rests in the upper left corner of the ``uneliminated'' matrix.
In the elimination phase, all the elements in the leftmost column and
below the pivot are set to zero by summing to each row a suitable
multiple of the pivot row.  Then the procedure is recursively applied
to the portion of the matrix excluding the topmost row and the
leftmost column.

Several parallel Gaussian Elimination algorithms have been proposed;
however, while the elimination step is readily parallelized, the
pivoting step is not, for it essentially involves collective
(blocking) communication among all involved processors.  Therefore,
some authors have proposed variants of the algorithm that perform
pivoting in some restricted form or no pivoting at all \cite{Tiskin99,
  DBLP:journals/corr/abs-0912-2047}, others have applied graph
partitioning algorithms to reorder the entries of the matrix so to
avoid pivoting during Gaussian Elimination \cite{grigoridemmelli07}.
A problem with these approaches is that they impose some restriction
on the class of matrices to which the algorithm applies: depending on
the algorithm variant, matrices need to be symmetric positive
definite, or diagonally dominant, or non-singular.  In addition, when
exact computations are wanted (e.g., over finite fields or integer
arithmetic), some of the above Gaussian Elimination algorithms fail to
be effective: during elimination, entries which are in pivot position
may become zero.  Gaussian Elimination algorithms with exact
computations has been analysed in \cite{Dumas02computingthe}; the
authors however concluded that there was ---to date--- no practical
parallel algorithm for computing the rank of sparse matrices.

Let us say that a matrix is in ``block echelon form'' iff each row
starts with a number of null entries that is \emph{not less} than the
number of null entries of the row directly above.
The algorithm presented here is based on the observation that any
sparse matrix can be put in a ``block echelon form'' with minimal
computational effort.  One can then run elimination on each block of
rows of the same length independently (i.e., in parallel); a
communication step is needed to re-order the rows after elimination. 
The procedure ends when all blocks have been reduced to a single row,
i.e., the matrix has been put in row echelon form.

As can be seen from this cursory description, the ``Rheinfall'' algorithm
distributes the matrix data and the elimination work to an arbitrary
number $P$ of processors; it can thus fully exploit the power of
present-day massively parallel
machines.  No collective communication takes place; however it is
assumed that the communication fabric is able to route a message of
arbitrary size\footnote{Actually, the maximum size of a message is a
  linear function of the length of a matrix row.} from any processor
to any other.  Numerical stability of the algorithm presented here is
equivalent to the numerical stability of serial Gaussian Elimination
with pivoting restricted to a column.

The ``Rheinfall'' algorithm has been developed and used to compute the
rank of the homology matrices arising from fatgraph complexes;
however, it is of (potentially) much wider application.  Indeed, we
present two variants of the algorithm, one for computing the rank of a
matrix and one for solving a linear system by $LU$ factorization.
Although parallel in nature, ``Rheinfall'' can also be run in a purely
sequential fashion; experimental results show that it can run faster
than classical pivoting Gaussian Elimination algorithms for certain
classes of matrices.

Section~\ref{sec:algo} describes the algorithm in detail and proves
its correctness.  A concrete parallel implementation using
OpenMP and MPI is presented in Section~\ref{sec:impl};
section~\ref{sec:results} compares the running time of this code with
analogous algorithms from the freely-available linear algebra package
LinBox \cite{linbox, linbox:website}; scalability results are discussed in the
following section.


\section{Description of the Algorithm}
\label{sec:algo}

The ``Rheinfall'' Gaussian Elimination algorithm presented here
comprises the parallel execution of the same code by several stateful
``Processing Units'' (PU), which perform the actual computation, and a
``master'' process that feeds initial data to the Processing Units,
and collects the results at the end of processing.  The master is not
involved in other phases of the computation, and remains inactive
while the PUs are running.

We shall first discuss the Gaussian Elimination algorithm for reducing
a matrix to row echelon form; practical applications like
computing matrix rank or computing the $LU$ decomposition will follow 
by simple modifications.
\begin{Algorithm}
  \caption{Reduce a matrix to strict row echelon form by Gaussian
    Elimination. \emph{Top:} Algorithm run by processing unit $P[c]$.
    \emph{Bottom:} Sketch of the ``master'' procedure.  Input to the
    algorithm is an $n \times m$ matrix $A$, represented as a list of
    rows $r_i$. Row and column indices are $0$-based.}
  \label{alg:echelon}
  \begin{codebox}
    \Procname{$\proc{ProcessingUnit}(c)$: [where $c$ is a column index]}
    \li $u \gets \const{nil}$ \RComment PU initialization
    \li $\id{output} \gets 0$
    \li $\id{state} \gets \const{running}$
    \li \While \id{state} is \const{running}          \label{li:lu:main}
    \RComment Main loop
    \li \Do wait for a message to arrive
    \li   \If received message \const{Row} with payload $r$
    \li   \Then
    \li     \If $u$ is \nil
    \li     \Then 
              $u \gets r$
    \li     \Else \label{li:pu:elimination}
              $r' \gets \proc{eliminate}(r,u)$
    \li       \If $r'$ is a null row 
    \li       \Then
                discard it and continue from line~\ref{li:lu:main}
              \End % \If $r_i$ null
    \li       $c' \gets$ first nonzero col.~of $r'$
    \li       send $r'$ to $P[c']$
            \End
    \li   \ElseIf received message \const{End}
    \li   \Then 
            wait for all sent messages to arrive
    \li     $\id{output} \gets u$                     \label{li:pu:result}
    \li     send \const{End} to $P[c+1]$
    \li     $\id{state} \gets \const{done}$
          \End% \If received ...
        \End% \While
    \li \Return \id{output}
    \end{codebox}
    \begin{codebox}
    \Procname{$\proc{Master}(A)$: [where $A$ is an $n\times m$ matrix]}
    \li start a PU $P[c]$ for each column $c$ of $A$  \label{li:master:start}
    \li \For i=0 \To n-1                              \label{li:master:read1}
    \li \Do
    \li   \If $r_i$ is a null row 
    \li   \Then
            skip it and continue with next $i$
          \End % \If $r_i$ null
    \li   $c \gets$ first nonzero column of $r_i$
    \li   send $r_i$ to $P[c]$                        \label{li:master:read2}
        \End % \For ...
    \li send \const{End} message to $P[0]$            \label{li:master:core1}
    \li wait until $P[m-1]$ receives the \const{End} message
                                                      \label{li:master:core2}
    \li \id{result} $\gets$ collect \id{output} results from all PUs
                                                      \label{li:master:result}
    \li \Return \id{result}                           \label{li:master:end}
  \end{codebox}
\end{Algorithm}

Let $A = (a_{ij} | i = 0, \ldots, n-1; j = 0, \ldots, m-1)$ be a $n
\times m$ matrix with entries in a ``sufficiently good'' ring
$\fk$ (a field, or a unique factorization domain or a principal
ideal domain).  Let $r_i := (a_{ij})_{j = 0, \ldots, m-1}$ be the
$i$-th row vector of $A$; we shall use the array notation $r_i[j]$ to
mean the element at column $j$ in row~$r_i$, i.e., $r_i[j] = a_{ij}$.
Define:
\begin{equation*}
  s_i := 
  \begin{cases}
    -1 &\text{if $a_{i*}$ is a null row,}
    \\
    \min \{j | a_{ij} \not= 0 \} &\text{otherwise.}
  \end{cases}
\end{equation*}
So, $s_i$ is the column index of the first non-zero entry in a
non-null row~$r_i$.
\begin{definition}
  The matrix $A$ is in \emph{block echelon form} iff $s_i
  \geq s_{i-1}$ for all $i = 1, \ldots, n-1$.

  $A$ is in \emph{row echelon form} iff $s_i$ is a
  \emph{strictly increasing} monotone function of $i$.
\end{definition}
Every matrix can be put in block
echelon form by a permutation of the rows.

We shall show that the code in Algorithm~\ref{alg:echelon} can be used
to transform any given matrix into row echelon form.

For reducing the $n\times m$ matrix $A$ to row echelon form,
$m$ ``Processing Units''\footnote{PU for short} $P[0], \ldots, P[m-1]$ will
be started.  Each PU independently runs the code in procedure \proc{ProcessingUnit}
(cf.~Algorithm~\ref{alg:echelon}); upon reaching the
\const{done} state, it returns its \id{output} to a ``master''
collector process, that assembles the global result.  The ``master''
process runs the \proc{Master} procedure in
Algorithm~\ref{alg:echelon}, and the PUs run the \proc{ProcessingUnit}
one.

There is one PU for each column of the matrix to be processed;
therefore, each PU also holds an implicit reference to the column it
is bound to, whose index will be available as the constant $c$.
Columns are numbered from 0 to $m-1$; Processing Unit $P[c]$ is the PU
handling rows starting at column $c$. It handles only rows of
length $m - c$.

Each Processing Unit maintains some internal state variables:
\begin{description}
\item[$u$] holds either a matrix row or the special value \const{nil}
  (indicating it has not been initialized yet);
\item[\id{output}] the contribution of this PU to the global result:
  its actual form varies with the specifics of what is being computed
  by the algorithm.  For reducing a matrix to row echelon form,
  \id{output} is a matrix row; when computing matrix rank, \id{output}
  of $P[c]$ is an integer value (0 or 1), indicating whether there is
  a non-null row of length $m-c$; finally, when computing the $LU$
  decomposition, \id{output} is a pair $(l, u)$ of matrix rows.
\item[\id{state}] a toggle indicating whether the processing unit is
  actively running or has terminated execution; the two possible states
  are represented by the constants \const{running} and \const{done};
\end{description}

A Processing Unit may send a message to every other PU; messages can
be of two sorts: \const{Row} messages and \const{End} messages.  The
payload of a \const{Row} message is a (portion of a) matrix row $r$, extending from
column $c$ to $m-1$; \const{End} messages carry no payload and just
signal the PU to finalize computations and stop execution.  We make
the assumption that messages sent from one PU to another arrive in the
same order they were sent.  In addition, we assume that it is possible
for a PU to wait until all the messages it has sent have been
delivered.  Both conditions are satisfied by MPI-compliant message
passing systems.

The \proc{eliminate} function at line~\ref{li:pu:elimination} in
Algorithm~\ref{alg:echelon} returns a sum of suitable multiples of $r$
and $u$ so that the resulting row has a null entry in all columns $j
\leq c$; this requires slightly different definitions, depending on
the coefficient ring of $A$.  For Gaussian Elimination over a field,
\proc{eliminate} returns $r - (r[c] / u[c]) u$; elimination over the
integer ring involves finding $\alpha, \beta \in \setZ$ such that $\alpha
r[c] = \beta u[c]$ and then setting $r' \gets \alpha r - \beta u$.
Note that $u[c] \not= 0$ by construction.

The ``master process'' is responsible for starting the $m$ independent
Processing Units $P[0]$, \ldots, $P[m-1]$, feeding the matrix data to
the processing units at the beginning of the computation (discarding
null rows), and sending the initial \const{End} message to PU $P[0]$.
When the \const{End} token reaches the last Processing Unit, the
computation is done and the master collects the results.

Lines~\ref{li:master:read1}--\ref{li:master:read2} in \proc{Master}
are responsible for putting the input matrix $A$ into block
echelon form. Indeed, the incoming message queue of $P[c]$ is formed
by matrix rows starting at column $c$; therefore, if we immediately
freeze the execution and output the message queues of $P[0]$, then
$P[1]$, etc., we get a matrix $A'$ which has the same rows as $A$
(albeit in a different order, and excluding null rows) and is in block echelon form.

Note that Lines~\ref{li:master:read1}--\ref{li:master:read2} can also be
effected in a different way: for instance, the Processing Units could
be created with the corresponding matrix rows already waiting in the
message queue.  

\begin{theorem}
  Algorithm~\ref{alg:echelon} reduces any given input matrix $A$ to
  row echelon form in finite time.
\end{theorem}
\begin{proof}
  The proof is by induction on $n$, the number of rows in the matrix
  $A$.  We can assume that $A$ has no null rows.

  If $n=1$, then $A$ is already in row echelon form; no
  Processing Unit will do any elimination, and the execution time is
  the traversal time of the \const{End} message from $P[0]$ to
  $P[m-1]$.

  Now assume $n > 1$; there is no loss in generality to assume that
  $A$ has at least one row starting at column $0$.  Up to a
  permutation of the rows, we can assume that $A$ is in block
  echelon form, and that rows $r_0$, \ldots, $r_k$ start at column
  $0$. Processing Unit $P[0]$ will do elimination on $r_0$, \ldots,
  $r_k$, and \emph{then} receive the \const{End} signal (because
  messages arrive in the exact order they were sent) within time
  $\tau_0$.  When $P[0]$ transitions to the \const{done} state, it will
  have retained one row (say, $r_0$) into the $u$ register, and sent
  rows $r'_1$, \ldots, $r'_k$ (i.e., $r_1$, \ldots, $r_k$ modified by
  elimination) to other PUs. By induction, the matrix $A'$ formed by
  $r'_1$, \ldots, $r'_k$ and rows $r_{k+1}$, \ldots, $r_n$ of $A$ will
  be put into row echelon form $(r''_1, \ldots, r''_n)$ by
  Algorithm~\ref{alg:echelon} in finite time $\tau_1$.  Since no
  $r''_i$ can start at column 0, $(r_0, r''_1, \ldots, r''_n)$ is in
  row echelon form, and the total running time is $\tau_0 +
  \tau_1 < \oo$.
\end{proof}

\subsection{Time Complexity Analysis}
\label{sec:complexity}

The ``Rheinfall'' algorithm operates closely like the serial Gaussian
Elimination, its strength being chiefly that each processing unit can
independently perform elimination on a subset of the rows, and that
communication can be completely overlapped with computation (to the
extent allowed by practical implementations).
The processing units are (logically) totally independent processes,
reacting to each other's messages; it is thus quite difficult to
reason about their collective performance.  

However, the total running time is easily described: it is the time it
takes for the \id{End} message to travel from the first PU $P[0]$ past
the last PU $P[m-1]$: we have $T = \sum_{c=0, \ldots, m-1} T_c$, where
$T_c$ is the time elapsed from the moment $t_c$ the \id{End} message
enters $P[c]$ message queue, until the moment $t_{c+1}$ when \id{End}
message until the \id{End} message sent by $P[c]$ reaches $P[c+1]$.
We can further decompose $T_c = T_c' + T_c'' + T_c'''$, where:
\begin{itemize}
\item $T_c'$ is the \emph{computation} time: it is the time it takes
  for $P[c]$ to process the backlog of \id{Row} messages that are in
  the message queue at the moment $t_c$ that \id{End} arrives;
\item $T_c'' + T_c'''$ is the \emph{communication} time: it is the
  time it takes for $P[c]$ to \emph{wait} for all the \id{Row}
  messages it has sent to arrive ($T_c''$), and the time to send the
  \id{End} message to $P[c+1]$ and wait until its arrival ($T_c'''$).
\end{itemize}
In most practical implementations, $T_c'''$ will only be dependent on
the network topology and placement of $P[c]$ and $P[c+1]$ in the
processing array; averaging over a series of independent runs, it can
be approximated by a constant $\tau$.  

Estimation of $T_c'$ and $T_c''$ is more difficult: both are
proportional to the number and size of \id{Row} messages in the
backlog of $P[c]$ at $t_c$; the author has not been able to find any
way to predict this number given an input matrix.  Indeed, one can run
all PUs on the same computer in a sequential fashion: $P[c+1]$ starts
its operation when $P[c]$ is done; in this setting, communication
times are negligible\footnote{Delivery of a message reduces to adding
  an item to an in-memory list.} so the total running time reduces to
the sum of computation times and is proportional to the number of row
eliminations performed.  A look at Table~\ref{table:XXX} shows that
even matrices with similar characteristics (e.g., the \texttt{Mgn}
family) can require vastly different processing times.\footnote{In
  other words, the running time over matrices of the same family does
  not correlate with the matrix dimensions, nor with the number of
  nonzero entries.}

The worst-case performance is actually attained on the $n \times
(n+1)$ matrix $\Lambda$ defined by
\begin{equation*}
  \Lambda_{ij} =
  \begin{cases}
    1  \qquad \text{if $j=0$ or $i = j-1$,}
    \\
    0  \qquad \text{otherwise.}
  \end{cases}
\end{equation*}
It is readily seen that the ``Rheinfall'' elimination of $\Lambda$
requires $O(n^3)$ operations (plus communication costs), just like
ordinary sequential Gaussian Elimination.\footnote{Experimental
  evidence suggests, however, that the algorithm is instead quite fast
  on a large number of real-world matrices. See
  Section~\ref{sec:results}.}

On the other hand, on the identity matrix $I_n$, no computation is
done and no \id{Row} messages are exchanged; thus, $T_c' + T_c'' = 0$
and $T \simeq \sum T_c'''$ is just the total time needed for exchange
of \id{End} messages.  In a sequential implementation, where the cost
of message-passing is negligible, we indeed find $T \simeq 0$.


\subsection{Variant: computation of matrix rank}
\label{sec:rank}

The Gaussian Elimination algorithm can be easily adapted to compute
the rank of a general (unsymmetric) sparse matrix: one just needs to
count the number of rows of the strict row echelon form.

Function \proc{ProcessingUnit} in Algorithm~\ref{alg:echelon}
is modified to return an integer number: the result shall be $1$ if at
least one row has been assigned to this PU ($u \not= \nil$) and $0$
otherwise.

Procedure \proc{Master} performs a sum-reduce when collecting results:
replace line~\ref{li:master:result} with $\id{result} \gets$
sum-reduce of \id{output} from $P[c]$, $c=1, \ldots, m$.


\subsection{Variant: $LU$ decomposition}
\label{sec:lu}

More extensive modifications are required for computing the $LU$
decomposition: the $U$ matrix is just the row echelon form of $A$, but
we need to build the matrix $L$ as we perform elimination on the rows.

For the rest of this section we assume that $A$ has coefficients in a
field.

Given an input matrix $A$, our $LU$ decomposition algorithm will
output a pair $(U, L\inv)$, where $U$ is the strict row echelon form
of $A$, and $L\inv$ is a lower triangular matrix with unitary
diagonal.  These data can be used to solve a linear system $Ax = b$ by
setting $b' = L\inv b$, and then solving $Ux = b'$ by
back-substitution.  Not every $A$ admits an $LU$
decomposition \cite{okunev+johnson:2005}: if any null rows arises
during Gaussian Elimination, the $LU$ decomposition algorithm fails.
\begin{Algorithm}
  \caption{Compute the $LU$ decomposition of a matrix by Gaussian
    Elimination. \emph{Top:} Algorithm run by processing unit $P[c]$.
    \emph{Bottom:} Sketch of the ``master'' procedure.  Input to the
    algorithm is an $n \times n$ matrix $A$, represented as a list of
    rows $r_i$. Row and column indices are $0$-based.}
  \label{alg:lu}
  \begin{codebox}
    \Procname{$\proc{ProcessingUnit}(c)$: [where $c$ is a column index]}
    \li $u \gets \const{nil}$ \RComment PU initialization
    \li $\id{output} \gets 0$
    \li $\id{state} \gets \const{running}$
    \li \While \id{state} is \const{running} 
    \RComment Main loop
    \li \Do wait for a message to arrive              \label{li:lupu:main}
    \li   \If received \const{Row} with payload $(r,l)$
    \li   \Then
    \li     \If $u$ is \nil
    \li     \Then 
              $u \gets r$ 
    \li       $v \gets l$
    \li     \Else                                     \label{li:lupu:elimination}
              $\alpha \gets r[c] / u[c]$
    \li       $r' \gets r - \alpha u$
    \li       \If $r'$ is a null row 
    \li       \Then
                discard it and continue from line~\ref{li:lupu:main}
              \End % \If $r_i$ null
    \li       $c' \gets$ first nonzero col.~of $r'$
    \li       $l[h(u)] \gets -\alpha$
    \li       send $(r', l)$ to $P[c']$
            \End
    \li   \ElseIf received message \const{End}
    \li   \Then 
            wait for all sent messages to arrive
    \li     $\id{output} \gets (u, v)$                \label{li:lupu:result}
    \li     send \const{End} to $P[c+1]$
    \li     $\id{state} \gets \const{done}$
          \End% \If received ...
        \End% \While
    \li \Return \id{output}
    \end{codebox}
    \begin{codebox}
    \Procname{$\proc{Master}(A)$: [where $A$ is an $n\times m$ matrix]}
    \li start a PU $P[c]$ for each column $c$ of $A$  \label{li:lumaster:start}
    \li \For i=0 \To n-1                              \label{li:lumaster:read1}
    \li \Do
    \li   \If $r_i$ is a null row 
    \li   \Then
            skip it and continue with next $i$
          \End % \If $r_i$ null
    \li   $c \gets$ first nonzero column of $r_i$
    \li   $l_i \gets (0, \ldots, 1, \ldots, 0)$
              \RComment{At $i$-th place}
    \li   send $(r_i, l_i)$ to $P[c]$                    \label{li:lumaster:read2}
        \End % \For i=0 ...
    \li send \const{End} message to $P[0]$            \label{li:lumaster:core1}
    \li wait until $P[m-1]$ receives the \const{End} message
                                                      \label{li:lumaster:core2}
    \li $(U, L\inv) \gets$ assemble \id{output} rows from all PUs
                                                      \label{li:lumaster:result}
    \li \Return \id{result}                           \label{li:lumaster:end}
  \end{codebox}
\end{Algorithm}

The modified algorithms works by exchanging pairs of rows $(r,l)$
among PUs.  The $l$ rows start out as rows of the identity matrix;
each time an elimination step is performed on $r$, the corresponding
operation is ``recorded'' in the paired $l$, so that it can later be
``replayed'' on the data vector $b$.

In order to correctly construct and assemble back matrices $U$ and
$L\inv$, we need to keep track of which row index a certain row has in
the original matrix $A$.  The original row index of $u$ is represented
in the listing of Algorithm~\ref{alg:lu} as a function $h(u)$; in a
practical implementation this would rather be a field into whatever
structure is used to represent a matrix row. It is understood that
the \proc{Master} procedure will collect pairs of rows $(u,l)$
from PUs and then construct $U$ and $L\inv$ by placing each $u$ into
$U$ at row index $c$ if $u$ came from $P[c]$,  and each $l$ into
$L\inv$ at row index $h(l)$.


\section{Implementation}
\label{sec:impl}

A C++ program has been written that implements matrix rank computation
with the variant of Algorithm~\ref{alg:echelon} previously described.
The same code can be compiled in a single-processor sequential
version, with OpenMP parallelization on multiple nodes, with MPI
messaging support for distributed computation over a cluster of
compute nodes, or finally in a hybrid MPI+OpenMP combination.  The
code is freely available for download and reuse from
\url{http://code.google.com/p/rheinfall}.

Input to the program is a matrix, represented as an array of
rows (\verb"std::vector<Row*>"); a sparse row is implemented as a C++
class wrapping an array of pairs formed by a column index and the
corresponding entry value
(\verb"std::vector<std::pair<coord_t,val_t>>").  An alternate dense
storage, implemented as a C++ class wrapping an array of values
(\verb"std::vector<val_t>"), is provided: PUs will automatically
switch from sparse to dense storage (on a row-by-row basis) if the
fill-in of a row exceeds a certain configurable threshold.  Both the
sparse and the dense row classes are derived from a common superclass
\verb"Row".

How to select the pivot row $u$ among the block of rows initially
assigned to a PU $P[c]$ has not been so far specified (it does not
affect correctness).  The actual implementation augments the outlined
procedure \textsc{ProcessingUnit}$[c]$ with the following rules:
\begin{enumerate}
\item Initially, the row with fewer nonzero entries is chosen as pivot
  row $u$.
\item At the arrival of each message \textsc{Row} with payload $r$,
  the contents of $u$ and $r$ are exchanged if $r$ has fewer
  entries than $u$.
\end{enumerate}
In a sparse row representation, only nonzero entries contribute to the
running time of the elimination step (line 10 in
\textsc{ProcessingUnit}$[c]$), so we compare the number of nonzero
entries in the above two rules; since each entry in a dense row
representation requires a mathematical operation during the
elimination phase , any entry in a dense row should be counted when
comparing the size of dense rows.  Experimental measurements show that
the above policy can result in a performance increment up to 30\%
compared with the na\"{\i}ve implementation.\FIXME{Rivedere.}

A convenience function can read a file in LinBox' SMS format (see
\cite{simc}) and dispatch rows directly to the responsible PU.  (Thus,
the input matrix is automatically put into block echelon form.)

Let $S$ be the number of MPI processes (ranks) available to the
program; each MPI rank starts $m/S$ PUs.  Since there is only a
limited degree of parallelism available on a single computing node,
Processing Units are not implemented as continuously-running threads;
rather, the \verb"ProcessingUnit" class provides a \verb"step()"
method, which does a single pass of the loop in procedure
\proc{ProcessingUnit}.  Each Processing Unit is thus equipped with a
message ``inbox'', i.e., a \verb"std::list<Row*>"; when a \id{Row}
message arrives, it is appended to the inbox; when a PU starts its
\verb"step()" procedure, it will perform elimination on all rows in
the inbox and immediately send the modified rows to other PUs for
processing.

The number $S$ of independent MPI ranks and an integer parameter
$w > 0$ control the distribution of matrix blocks to worker processes.
The input matrix is divided into bands formed by $w$ adjacent columns;
bands are distributed to MPI processes in a cyclic-wrapped
fashion,\footnote{MPI process $s$ hosts the processing units $P[c],
  \ldots, P[c+w-1]$ belonging to the $(s + kS)$-th band, with $0 \leq
  s < S$, $k=0,1,\ldots$ and $c = w \cdot (s + kS)$.} in order to
balance the memory requirements\footnote{Processing Unit $P[c]$
  handles rows of length $m-c$.} and work load.  

The main loop of the program calls each PU's \verb"step()" in turn,
until all PUs have performed one round of elimination, after which,
messages are received for all PUs at the same time.  The main loop
corresponds approximately to the following pseudo-code:
\begin{codebox}
  \li $c_0 \gets \min\{c : P[c] \text{ is in \id{running} state}\}$
  \li \While $c_0 < m$
  \li \Do
  \li\label{li:main:4}   
        \For $c$ in $c_0, ..., m-1$ 
  \li   \Do
  \li\label{li:main:6}     
          call \texttt{step()} on $P[c]$
        \End
  \li   receive messages
  \li   $c_0 \gets \min\{c : P[c] \text{ is in \id{running} state}\}$
      \End
\end{codebox}
When the main loop receives an \id{End} message, it flips the
destination PU's \id{state} variable to a transient \const{ending}
state: the subsequent invocation of \verb"step()" switches \id{state}
to \const{done} after having waited for all sent messages to be
delivered.

\subsection{Sequential version}
\label{sec:sequential-impl}

In a single-processor setting, there is no actual ``communication''
happening: in order to ``pass a message'' from a PU to another, it
suffices to move a (pointer to) a row from the inbox of one PU to the
other. 

In the main loop, each $P[c]$ is called in turn, one after the other.
Since $P[0]$ transitions to \id{done} state on the very first call to
\verb"step()", it immediately delivers an \id{End} message to $P[1]$;
then $P[1]$'s \verb"step()" is called and it transitions to \id{done},
delivering the \id{End} message to $P[2]$, and so on.  Therefore: the
inner loop at lines~\ref{li:main:4}--\ref{li:main:6} of the main loop
is only executed \emph{once}.

Thus, all ``communication'' costs are negligible and the total running
time is the time needed to perform elimination on all row blocks in
sequence.


\subsection{Single-node OpenMP}
\label{sec:openmp-impl}

In a single-node OpenMP setting capable of running $p$ independent
threads, the main loop is na\"{\i}vely modified by using an OpenMP
``parallel for'': each thread would call the the \verb"step" procedure
of a segment of $w$ adjacent PUs.

As in the sequential execution model, message delivery costs are
negligible.  However, $w$ caps the number of PUs that can turn to
\id{done} state in a single iteration: when $P[c+w-1]$ emits the
\id{End} message, $P[c+w]$'s \verb"step" has already been run by
another thread.  This introduces a significant delay in the processing
of the \id{End} message by $P[c+w]$, and can be accrued as a
non-negligible communication time $T_{c+w-1}'''$.

Experimental results show that the delay so introduced can be quite
substantial, and that the single-node OpenMP implementation is slower
than the sequential one for low values of $w$, and becomes only
marginally faster for higher values of $w$.


\subsection{MPI}
\label{sec:mpi-impl}

As in the OpenMP parallel threads implementation, the set of PUs is
partitioned into bands formed by $w$ adjacent PUs, and bands are
assigned to MPI processes in a round-robin fashion.  Communication
costs are negligible only when two PUs reside in the same MPI
process. 

Thus, $w$ influences the performance of the MPI version of the
algorithm; lower values of $w$ give better memory balance but
introduce a higher communication cost: since PUs are activated in the
main loop in ascending column index order, and the \id{End} message is
always passed from one PU to the next one, at most $w$ blocks can be
processed to final state in a single run of the main loop with
negligible communication cost.  Indeed, experimental results show an
increase in performance with increasing $w$, up to a point
(matrix-dependent), past which no further improvement is seen.

In addition, the MPI distributed memory model allows
``Rheinfall'' to process larger matrices that would not fit the memory
of any single computation node.


\section{Experimental results}
\label{sec:results}

\subsection{Sequential performance}
\label{sec:performance}

In order to compare the performance of the ``Rheinfall'' algorithm
described here to the freely-available implementation of Gaussian
Elimination in the LinBox library, we computed the rank of all the
(integer-valued) matrices available in the online collection ``SIMC''
\cite{simc}.  It should be noted that the computations run strictly on
a single CPU core (no parallel computation whatsoever).

...

Further research is needed to highlight the precise
conditions on a matrix $A$ to be the ``optimal target'' for the
algorithm described here.

Since, up to our knowledge, there are no other available
implementations of a parallel algorithm for computing integer matrix
rank, it is not possible to compare the parallel ``Rheinfall''
performance to other software.

\subsection{Parallel scalability}
\label{sec:scalability}

Scalability measurements have been done by running the ``Rheinfall''
MPI/C++ implementation on the single matrix \textsc{M06-D10} across,
respectively, 8, 16, 32, 64, 128, 256 and 512 processes.  Each
computing node allowed 8 processes to run concurrently; nodes are
connected by means of a quad-bandwidth InfiniBand link.

...

Results show almost perfect scalability with the increasing number of
MPI processes.

On the performance side, however, it should be noted that every MPI
run, however large the number of allocated CPU cores, cannot match the
speed of a single-node run, when all executing threads are located on
the same machine (and can thus communicate at ``zero latency'' and
``infinite bandwidth'').  Therefore, use of the distributed-memory
version of ``Rheinfall'' is only convenient when a matrix cannot be
processed on a single node, due to its size, and rather
\emph{requires} a distributed-memory approach.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "index"
%%% End: 
